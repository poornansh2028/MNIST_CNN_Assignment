{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c7cbb08",
   "metadata": {},
   "source": [
    "# MNIST Digit Recognition with CNN\n",
    "## Theoretical Foundations in Machine Learning\n",
    "## 1. Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede94fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2325c03",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing\n",
    "**Normalization:** Scaling pixel values to [0â€“1] helps stabilize gradient descent during training.\n",
    "\n",
    "**One-hot Encoding:** Converts class labels into binary matrix form for multi-class classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75120f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize\n",
    "X_train = X_train.reshape(-1, 28, 28, 1).astype('float32') / 255\n",
    "X_test = X_test.reshape(-1, 28, 28, 1).astype('float32') / 255\n",
    "\n",
    "# One-hot encode labels\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9423440",
   "metadata": {},
   "source": [
    "## 3. Conceptual Questions\n",
    "\n",
    "**Q1: Why use CNNs instead of traditional models like Random Forests or SVMs?**\n",
    "CNNs are specifically designed for image data. They capture spatial hierarchies using convolutional filters, whereas traditional models treat input as flat vectors and miss spatial information. CNNs are more parameter-efficient, translation-invariant, and generalize better on image data.\n",
    "\n",
    "**Q2: Why are non-linear activation functions essential? Which one is most appropriate here and why?**\n",
    "Non-linear activation functions allow networks to learn complex patterns beyond linear relationships. Without them, even deep networks act like linear models. **ReLU** is the most appropriate here because it avoids the vanishing gradient problem and accelerates convergence. **Softmax** is used in the output layer to convert outputs into class probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443ceeb4",
   "metadata": {},
   "source": [
    "## 4. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9797f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b5f33c",
   "metadata": {},
   "source": [
    "## 5. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c553f9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=64,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e80252",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1195def2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3e0c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training vs Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36711478",
   "metadata": {},
   "source": [
    "## 7. Overfitting Mitigation Strategies\n",
    "- **Data Augmentation**: Adding transformations like rotation, zoom, shift.\n",
    "- **Dropout Layers**: Randomly disabling neurons during training.\n",
    "- **L2 Regularization**: Penalizing large weights.\n",
    "- **Early Stopping**: Stopping training when validation loss stops improving."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d38a50",
   "metadata": {},
   "source": [
    "## 8. Hyperparameter Tuning Guide\n",
    "| Parameter       | Suggested Range |\n",
    "|-----------------|-----------------|\n",
    "| Learning Rate   | 1e-2 to 1e-5    |\n",
    "| Batch Size      | 32 to 256       |\n",
    "| Filter Sizes    | 32 to 128       |\n",
    "| Dense Units     | 64 to 512       |\n",
    "\n",
    "**Observations:**\n",
    "- Higher filters and dense units improve accuracy but may lead to overfitting.\n",
    "- Smaller learning rates lead to stable but slower convergence.\n",
    "- Batch size balances between memory and speed."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
